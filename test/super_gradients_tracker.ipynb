{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbe489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import cv2\n",
    "import torch\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "# 定义了一些命令行参数，使得你可以在运行脚本时通过命令行来传递特定的值\n",
    "flags.DEFINE_string('f', 'value', 'The explanation of this parameter')\n",
    "flags.DEFINE_string('model', 'yolo_nas_l', 'yolo_nas_l or yolo_nas_m or yolo_nas_s')\n",
    "flags.DEFINE_string('video', \"test.mp4\", 'path to input video or set to 0 for webcam')\n",
    "flags.DEFINE_string('output', \"output.mp4\", 'path to output video')\n",
    "flags.DEFINE_float('conf', 0.50, 'confidence threshhold')\n",
    "\n",
    "\n",
    "# 主函数，从命令行参数解析参数并执行处理逻辑\n",
    "def main(_argv):\n",
    "    \n",
    "    video_cap = cv2.VideoCapture(FLAGS.video)\n",
    "    #获得帧的宽度\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    #获得帧的高度\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    #获取视频的帧率（帧每秒）\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # 初始化视频写入对象\n",
    "    #创建一个FourCC（四字符代码）对象，用于指定视频编码格式\n",
    "    #FourCC 'MP4V' 表示使用 MPEG-4 Part 2 编码，通常用于生成MP4格式的视频文件。FourCC是一种用于标识视频编码格式的标准。\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    #创建一个名为 writer 的视频写入对象它接受四个参数：\n",
    "    #FLAGS.output: 这是之前定义的命令行参数，表示输出视频的路径和文件名。\n",
    "    #fourcc: 这是上一行创建的FourCC对象，指定了视频编码格式。\n",
    "    #fps: 视频的帧率，用于指定写入的视频的帧率。\n",
    "    #(frame_width, frame_height): 这是视频帧的宽度和高度，用于指定写入的视频的分辨率。\n",
    "    writer = cv2.VideoWriter(FLAGS.output, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # 初始化 DeepSort 跟踪器\n",
    "    #max_age 是一个参数，用于指定跟踪目标的最大帧数。如果目标在超过这个帧数之后仍然没有被检测到，它将被视为已失去跟踪，从跟踪器中移除。\n",
    "    tracker = DeepSort(max_age=50)\n",
    "\n",
    "    # 检查是否可用 GPU，否则使用 CPU\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # 加载 YOLO 模型\n",
    "    model = models.get(FLAGS.model, pretrained_weights=\"coco\").to(device)\n",
    "\n",
    "    # 加载 YOLO 模型所训练的 COCO 类别标签\n",
    "    classes_path = \"coco.names\"\n",
    "    #f.read() 读取了文件的内容，\n",
    "    #.strip() 方法去除了每行末尾的空白字符（如换行符），\n",
    "    #然后 .split(\"\\n\") 方法将文件内容按行分割成一个列表\n",
    "    with open(classes_path, \"r\") as f:\n",
    "        class_names = f.read().strip().split(\"\\n\")\n",
    "\n",
    "    # 创建一个随机颜色列表来表示每个类别\n",
    "    np.random.seed(42)  # 设置随机数种子，以确保每次运行生成的随机数相同\n",
    "    #生成随机数组，行数为目标类别数即为coco.names的行数，列数为3表示有三个颜色通道（红绿蓝），数值为0-255，每一行都是该类别颜色的RGB值\n",
    "    colors = np.random.randint(0, 255, size=(len(class_names), 3))  \n",
    "    # 在主函数内定义鼠标事件处理函数\n",
    "    selected_object_id = -1\n",
    "\n",
    "    def on_mouse_click(event, x, y, flags, param):\n",
    "        nonlocal selected_object_id\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:  # 当鼠标左键点击时\n",
    "            for idx, track in enumerate(tracks):\n",
    "                ltrb = track.to_tlbr()  # 左上角和右下角坐标\n",
    "                x1, y1, x2, y2 = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "                if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                    selected_object_id = idx\n",
    "                    print(f\"Selected object with ID: {selected_object_id}\")\n",
    "                    break\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # 调用当前时间语句记录开始时间以计算 FPS\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # 从视频捕获中读取一帧，设定ret布尔值来判断是否结束循环\n",
    "        ret, frame = video_cap.read()\n",
    "        #在这部分加入鼠标回应函数，一旦点击，ret值设置为0，跳出循环，执行接下来的循环，实现点击目标框专门跟踪某人的功能\n",
    "       \n",
    "        # 如果没有帧，说明已经到达视频末尾或鼠标选择\n",
    "        if not ret:\n",
    "            print(\"End of the video file...\")\n",
    "            break\n",
    "\n",
    "        # 对帧运行 YOLO 模型进行目标检测\n",
    "        detect = next(iter(model.predict(frame, iou=0.5, conf=FLAGS.conf)))\n",
    "\n",
    "        # 从检测结果中提取边界框坐标、置信度分数和类别标签\n",
    "        bboxes_xyxy = torch.from_numpy(detect.prediction.bboxes_xyxy).tolist()\n",
    "        confidence = torch.from_numpy(detect.prediction.confidence).tolist()\n",
    "        labels = torch.from_numpy(detect.prediction.labels).tolist()\n",
    "        \n",
    "        # 将边界框坐标和置信度分数合并为一个列表\n",
    "        concate = [sublist + [element] for sublist, element in zip(bboxes_xyxy, confidence)]\n",
    "        \n",
    "        # 将合并的列表与类别标签合并为最终的预测列表\n",
    "        final_prediction = [sublist + [element] for sublist, element in zip(concate, labels)]\n",
    "\n",
    "        # 初始化边界框和置信度列表\n",
    "        results = []\n",
    "        \n",
    "        # 遍历检测结果\n",
    "        for data in final_prediction:\n",
    "            confidence = data[4]  # 提取与检测相关的置信度\n",
    "\n",
    "            # 过滤掉置信度小于阈值的弱检测\n",
    "            if float(confidence) < FLAGS.conf:\n",
    "                continue\n",
    "\n",
    "            # 如果置信度大于阈值，将边界框绘制在帧上\n",
    "            xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "            class_id = int(data[5])\n",
    "            \n",
    "            # 将边界框（x、y、w、h）、置信度和类别 ID 添加到结果列表\n",
    "            results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "        # 使用新的检测结果更新跟踪器\n",
    "        tracks = tracker.update_tracks(results, frame=frame)\n",
    "        for idx, track in enumerate(tracks):\n",
    "            if selected_object_id != -1 and idx != selected_object_id:\n",
    "                continue\n",
    "            if selected_object_id != -1 and idx == selected_object_id:\n",
    "                # 在这里添加处理特定对象的代码\n",
    "                ltrb = track.to_ltrb()  # 获取跟踪信息\n",
    "                class_id = track.get_det_class()\n",
    "                \n",
    "\n",
    "                x1, y1, x2, y2 = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "                captured_image = frame[y1:y2, x1:x2]  # 截取图像\n",
    "                cv2.imwrite(\"captured_image.jpg\", captured_image)\n",
    "                # 删除选定对象的原有边界框和文本\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 255), 2)  # 用白色绘制边界框\n",
    "                cv2.putText(frame, \"\", (x1 + 5, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)  # 清空文本\n",
    "        # 目标对象框设置为红色\n",
    "                B, G, R = 0,0,255\n",
    "\n",
    "        # 创建显示在帧上的文本\n",
    "                class_name = class_names[class_id]\n",
    "                text = f\"{track_id} - {class_name}(Selected) \"\n",
    "\n",
    "        # 在帧上绘制边界框和文本\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (B, G, R), 2)\n",
    "                cv2.rectangle(frame, (x1 - 1, y1 - 20), (x1 + len(text) * 12, y1), (B, G, R), -1)\n",
    "                cv2.putText(frame, text, (x1 + 5, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "               \n",
    "        # 遍历跟踪结果\n",
    "        #for track in tracks:\n",
    "        for idx, track in enumerate(tracks):\n",
    "                # 如果跟踪未确认，忽略它\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            if selected_object_id != -1 and idx == selected_object_id:\n",
    "                continue\n",
    "\n",
    "            # 获取跟踪 ID 和边界框\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            class_id = track.get_det_class()\n",
    "            \n",
    "            x1, y1, x2, y2 = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "            # 获取类别的颜色\n",
    "            color = colors[class_id]\n",
    "            B, G, R = int(color[0]), int(color[1]), int(color[2])\n",
    "            \n",
    "            # 创建显示在帧上的文本\n",
    "            class_name = class_names[class_id]\n",
    "            text = f\"{track_id} - {class_name} \"\n",
    "\n",
    "            # 在帧上绘制边界框和文本\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (B, G, R), 2)\n",
    "            cv2.rectangle(frame, (x1 - 1, y1 - 20), (x1 + len(text) * 12, y1), (B, G, R), -1)\n",
    "            cv2.putText(frame, text, (x1 + 5, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "\n",
    "\n",
    "        # 记录结束时间以计算 FPS\n",
    "        end = datetime.datetime.now()\n",
    "        \n",
    "        # 显示处理 1 帧所需时间\n",
    "        print(f\"Time to process 1 frame: {(end - start).total_seconds() * 1000:.0f} milliseconds\")\n",
    "        \n",
    "        # 计算并绘制 FPS\n",
    "        fps = f\"FPS: {1 / (end - start).total_seconds():.2f}\"\n",
    "        cv2.putText(frame, fps, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 8)\n",
    "\n",
    "        \n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        # 调整窗口大小以适应视频分辨率\n",
    "        cv2.resizeWindow(\"Frame\", frame_width, frame_height)\n",
    "        # 显示帧\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        # 将帧写入输出视频文件\n",
    "        writer.write(frame)\n",
    "        cv2.setMouseCallback(\"Frame\", on_mouse_click)  # 设置鼠标事件回调函数\n",
    "       \n",
    "\n",
    "\n",
    "        # 检查是否按下 'q' 键来退出循环\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # 释放视频捕获和视频写入对象\n",
    "    video_cap.release()\n",
    "    writer.release()\n",
    "\n",
    "\n",
    "    # 关闭所有窗口\n",
    "    cv2.destroyAllWindows()\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(main)\n",
    "\n",
    "    except SystemExit:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15e938aa",
   "metadata": {},
   "source": [
    "这段代码是一个Python脚本，用于实现目标跟踪和识别。它使用了多个库和模块，包括numpy、datetime、opencv、torch、absl、deep_sort_realtime和super_gradients。\n",
    "\n",
    "代码的功能如下：\n",
    "\n",
    "导入必要的库和模块。\n",
    "\n",
    "定义命令行参数。\n",
    "\n",
    "--model：选择使用的模型，可以是'yolo_nas_l'、'yolo_nas_m'或'yolo_nas_s'。\n",
    "--video：输入视频的路径，可以是本地视频文件路径或摄像头设备的编号（设置为0表示使用摄像头）。\n",
    "--output：输出视频的路径。\n",
    "--conf：置信度阈值。\n",
    "创建DeepSort跟踪器对象，用于目标跟踪。\n",
    "\n",
    "根据选择的模型，加载相应的模型。\n",
    "\n",
    "初始化超级梯度框架的模型训练模块。\n",
    "\n",
    "根据命令行参数创建相应的对象。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91ba6332",
   "metadata": {},
   "source": [
    "首先，通过调用cv2.VideoCapture函数打开输入视频文件，并获取视频的宽度、高度和帧率。然后，使用cv2.VideoWriter函数创建一个视频写入对象，用于将处理后的视频帧写入输出文件。\n",
    "\n",
    "接下来，初始化了一个DeepSort对象，用于目标跟踪。然后，检查GPU是否可用，根据可用情况选择使用GPU还是CPU。\n",
    "\n",
    "使用models.get函数加载指定名称的模型，并使用预训练权重文件\"coco\"进行初始化。然后，加载COCO类标签文件，该文件包含了模型训练时使用的类别名称。\n",
    "\n",
    "接下来，使用np.random.randint函数生成一个随机颜色列表，每个颜色代表一个类别。\n",
    "\n",
    "最后，在主函数中调用以上初始化和配置的代码，并执行后续的目标检测和跟踪等操作。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7db829c",
   "metadata": {},
   "source": [
    "这段代码是一个用于视频对象检测的程序，它使用了YOLO（You Only Look Once）模型。主要流程如下：\n",
    "\n",
    "程序从视频文件中逐帧读取画面。\n",
    "对每一帧画面，使用YOLO模型进行对象检测，得到检测结果。\n",
    "从检测结果中提取边界框坐标、置信度、类别标签等信息。\n",
    "根据置信度过滤掉低信度的检测结果，只保留置信度高于设定阈值的检测结果。\n",
    "对于满足置信度要求的检测结果，在画面上绘制出对应的边界框，并标注出类别标签。\n",
    "循环过程会持续到视频文件结束。这个过程中，程序会计算并显示FPS（每秒帧数）。\n",
    "\n",
    "具体到代码，while True 创建了一个无限循环，不断读取和处理视频帧。start = datetime.datetime.now() 记录了开始处理当前帧的时间，用于计算FPS。\n",
    "\n",
    "ret, frame = video_cap.read() 读取视频的下一帧。if not ret: print(\"End of the video file...\") break 判断是否到达视频末尾，到达则结束循环。\n",
    "\n",
    "model.predict(frame, iou=0.5, conf=FLAGS.conf) 使用YOLO模型对帧进行对象检测。预测结果包含边界框坐标、置信度和类别标签等信息。\n",
    "\n",
    "接下来的代码处理检测结果，包括过滤低信度检测、绘制边界框等操作。\n",
    "\n",
    "最后，video_cap.release() 释放视频捕捉对象，cv2.destroyAllWindows() 关闭所有创建的窗口。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8707be0",
   "metadata": {},
   "source": [
    "这段代码是一个使用OpenCV库的Python程序，它使用YOLO（You Only Look Once）模型进行视频中的对象检测，并使用DeepSort模型进行对象跟踪。\n",
    "\n",
    "主要流程如下：\n",
    "\n",
    "从视频文件中逐帧读取画面。\n",
    "使用YOLO模型对每一帧画面进行对象检测，得到检测结果。\n",
    "过滤掉置信度低于设定阈值的低信度检测结果，只保留置信度高于设定阈值的检测结果。\n",
    "对于满足置信度要求的检测结果，在画面上绘制出对应的边界框，并标注出类别标签。\n",
    "更新跟踪器（tracker）的跟踪轨迹（tracks）。\n",
    "循环遍历跟踪轨迹。对于每个跟踪轨迹，如果它没有被确认，就忽略它。否则，获取跟踪轨迹的ID和边界框，并获取该对象的类别ID。\n",
    "根据类别ID获取对应的颜色，然后在画面上绘制边界框和对应的文本信息。\n",
    "计算处理每一帧画面的时间，并显示出来。\n",
    "计算并显示FPS（每秒帧数）。\n",
    "显示处理后的画面。\n",
    "将处理后的画面写入输出视频文件。\n",
    "如果用户按下“q”键，就退出循环。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cdf49ee",
   "metadata": {},
   "source": [
    "在程序结束时，你释放了视频捕捉对象（video_cap）和视频写入对象（writer），然后关闭了所有OpenCV的窗口。\n",
    "\n",
    "__name__ == '__main__'是一个常见的Python模式，用于确保某个代码块只在脚本被直接运行时执行，而不是在作为模块导入时执行。\n",
    "\n",
    "try/except块用于捕获和处理异常。在你的代码中，SystemExit是一个由sys.exit()或Python解释器退出程序引发的异常。如果程序正常结束，那么这个异常会被捕获并忽略（实际上，它会被传递给程序的父进程或操作系统）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
